{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqfH7pF5Q_ev"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as pl\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csaPtOLYROou"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"/content/drive/MyDrive/ASD/Dataset/Toddler Autism dataset July 2018 (1).csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUSYbbz3Q10k"
      },
      "outputs": [],
      "source": [
        "df.Sex.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBLfaEd2ROl8"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pK8ul9g4ROj-"
      },
      "outputs": [],
      "source": [
        "df=df.drop(\"Qchat-10-Score\",axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dmiMi2nROhz"
      },
      "outputs": [],
      "source": [
        "df['Age_Mons'] = df['Age_Mons'] / 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2cpFyHTROft"
      },
      "outputs": [],
      "source": [
        "df.columns = df.columns.str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Gsb4YiVROdn"
      },
      "outputs": [],
      "source": [
        "columns_to_encode = ['Sex','Jaundice','Class/ASD Traits','Family_mem_with_ASD']\n",
        "label_encoder = LabelEncoder()\n",
        "for column in columns_to_encode:\n",
        "    df[column] = label_encoder.fit_transform(df[column])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOD5PPuNR18f"
      },
      "outputs": [],
      "source": [
        "df.rename(columns={'Who completed the test': 'Who_completed_the_test'}, inplace=True)\n",
        "df.rename(columns={'Age_Mons': 'Age'}, inplace=True)\n",
        "df.rename(columns={'Family_mem_with_ASD': 'autism'}, inplace=True)\n",
        "df.rename(columns={'jaundice': 'Jaundice'}, inplace=True)\n",
        "    # Rename columns A1, A2, ..., A10 to A1_score, A2_score, ..., A10_score\n",
        "new_columns = {f'A{i}': f'A{i}_score' for i in range(1, 11)}\n",
        "df.rename(columns=new_columns, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyPJmJ3JROZi"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Us2fP0qLRObi"
      },
      "outputs": [],
      "source": [
        "df.rename(columns={'Age_Mons': 'Age'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsc2WxzvSWoL"
      },
      "outputs": [],
      "source": [
        "df.to_csv('Toddler.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqfn5at_x8n1"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5i-elgcSfoZ"
      },
      "source": [
        "Child Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU9JhX11SgvM"
      },
      "outputs": [],
      "source": [
        "df1=pd.read_csv(\"/content/drive/MyDrive/ASD/Dataset/csv_result-Autism-Child-Data.csv\")\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PPewyxYSs6u"
      },
      "outputs": [],
      "source": [
        "df1.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrSBGv3BQ_-8"
      },
      "outputs": [],
      "source": [
        "df1.gender.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goh5H0woSs3d"
      },
      "outputs": [],
      "source": [
        "df1.rename(columns={'jundice': 'jaundice'}, inplace=True)\n",
        "df1.rename(columns={'austim': 'autism'}, inplace=True)\n",
        "df1.rename(columns={'age': 'Age'}, inplace=True)\n",
        "df1.rename(columns={'jaundice': 'Jaundice'}, inplace=True)\n",
        "df1.rename(columns={'gender': 'Sex'}, inplace=True)\n",
        "df1.rename(columns={'ethnicity': 'Ethnicity'}, inplace=True)\n",
        "df1.rename(columns={'Class/ASD': 'Class/ASD Traits '}, inplace=True)\n",
        "print(df1.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyYRm5xJSs2H"
      },
      "outputs": [],
      "source": [
        "df1['Age']=pd.to_numeric(df1['Age'],errors='coerce')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CulZN5_rB_Zz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZWXxx94Ss0P"
      },
      "outputs": [],
      "source": [
        "columns_to_encode = ['Age','Sex', 'Jaundice',  'Class/ASD Traits ','autism']\n",
        "label_encoder = LabelEncoder()\n",
        "for column in columns_to_encode:\n",
        "    df1[column] = label_encoder.fit_transform(df1[column])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddG4OojEadaC"
      },
      "outputs": [],
      "source": [
        "df1.drop(['age_desc','id','result','used_app_before','contry_of_res','relation','result'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS3Lo4WQbaI4"
      },
      "outputs": [],
      "source": [
        "df1['Ethnicity'].replace('?', 'Others', inplace=True)\n",
        "df1['Ethnicity'].replace('Middle Eastern ','Middle Eastern',inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJlqQl6lSsyJ"
      },
      "outputs": [],
      "source": [
        "df1.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXfpoDyzbxjD"
      },
      "outputs": [],
      "source": [
        "unique_values = df1['Ethnicity'].unique()\n",
        "print(unique_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoocXrN5azYc"
      },
      "outputs": [],
      "source": [
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zrqJewMSsv4"
      },
      "outputs": [],
      "source": [
        "df1.to_csv('Child_Data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kR8yViKfz0o"
      },
      "source": [
        "#Adolescent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnIdT69oeZSA"
      },
      "outputs": [],
      "source": [
        "df2=pd.read_csv(\"/content/drive/MyDrive/ASD/Dataset/csv_result-Autism-Adolescent-Data.csv\")\n",
        "df2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYA2NU3BeZJK"
      },
      "outputs": [],
      "source": [
        "df2.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUUGsYdIRKhY"
      },
      "outputs": [],
      "source": [
        "df2.gender.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAi1WtUPeY5K"
      },
      "outputs": [],
      "source": [
        "df2.rename(columns={'jundice': 'Jaundice'}, inplace=True)\n",
        "df2.rename(columns={'age': 'Age'}, inplace=True)\n",
        "df2.rename(columns={'gender': 'Sex'}, inplace=True)\n",
        "df2.rename(columns={'austim': 'autism'}, inplace=True)\n",
        "df2.rename(columns={'ethnicity': 'Ethnicity'}, inplace=True)\n",
        "df2.rename(columns={'Class/ASD': 'Class/ASD Traits '}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6rDGK9teY0t"
      },
      "outputs": [],
      "source": [
        "df2['Ethnicity'].replace('?', 'Others', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELVrwc_BgWsz"
      },
      "outputs": [],
      "source": [
        "df2.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AJ84qb5gq6r"
      },
      "outputs": [],
      "source": [
        "df2.drop(['age_desc','id','result','used_app_before','contry_of_res','relation','result'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ra_Qnstsv70g"
      },
      "outputs": [],
      "source": [
        "columns_to_encode = ['Sex' ,'Jaundice','Class/ASD Traits ','autism']\n",
        "label_encoder = LabelEncoder()\n",
        "for column in columns_to_encode:\n",
        "    df2[column] = label_encoder.fit_transform(df2[column])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPmtW2k1g6iY"
      },
      "outputs": [],
      "source": [
        "df2.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-WnruPwg85M"
      },
      "outputs": [],
      "source": [
        "df2.to_csv('Adolescent_Data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ra81I6EhL2N"
      },
      "source": [
        "Adult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LFEiFY2hNtb"
      },
      "outputs": [],
      "source": [
        "df3=pd.read_csv(\"/content/drive/MyDrive/ASD/Dataset/csv_result-Autism-Adult-Data.csv\")\n",
        "df3.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDsMyDEPhVc2"
      },
      "outputs": [],
      "source": [
        "df3.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PN6urLcDRRSp"
      },
      "outputs": [],
      "source": [
        "df3.gender.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pADa02DshOZP"
      },
      "outputs": [],
      "source": [
        "df3['age']=pd.to_numeric(df3['age'],errors='coerce')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZmiZW2ehOUk"
      },
      "outputs": [],
      "source": [
        "replacements = {\n",
        "      '?' : 'Others'\n",
        "}\n",
        "df3['ethnicity'] = df3['ethnicity'].replace(replacements)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRk2nr5PhOPx"
      },
      "outputs": [],
      "source": [
        "columns_to_encode = ['gender', 'jundice', 'austim', 'contry_of_res',\n",
        "       'used_app_before', 'result', 'age_desc', 'relation', 'Class/ASD',]\n",
        "label_encoder = LabelEncoder()\n",
        "for column in columns_to_encode:\n",
        "    df3[column] = label_encoder.fit_transform(df3[column])\n",
        "df3.rename(columns={'jundice': 'Jaundice'}, inplace=True)\n",
        "df3.rename(columns={'gender': 'Sex'}, inplace=True)\n",
        "df3.rename(columns={'ethnicity': 'Ethnicity'}, inplace=True)\n",
        "df3.rename(columns={'age': 'Age'}, inplace=True)\n",
        "df3.rename(columns={'austim': 'autism'}, inplace=True)\n",
        "\n",
        "df3.rename(columns={'Class/ASD': 'Class/ASD Traits '}, inplace=True)\n",
        "# Drop specified columns\n",
        "columns_to_drop = [ 'contry_of_res', 'used_app_before', 'result', 'age_desc', 'relation']\n",
        "df3.drop(columns=columns_to_drop, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsX23Hd3juYu"
      },
      "outputs": [],
      "source": [
        "df3.drop(['id',],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fhwi6CPBhONz"
      },
      "outputs": [],
      "source": [
        "df3.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kFKAH56hOLd"
      },
      "outputs": [],
      "source": [
        "df3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnmM3nBjhOJH"
      },
      "outputs": [],
      "source": [
        "df3.to_csv('Adult_Data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQza6Be3wuYi"
      },
      "source": [
        "Combining Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUpPH8L5hOEt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Paths of the CSV files\n",
        "csv_files = [\n",
        "    '/content/Toddler_Data.csv',\n",
        "    '/content/Child_Data.csv',\n",
        "    '/content/Adolescent_Data.csv',\n",
        "    '/content/Adult_Data.csv'\n",
        "]\n",
        "\n",
        "# Function to standardize column names\n",
        "def standardize_columns(df):\n",
        "    df.columns = df.columns.str.lower().str.strip()\n",
        "    return df\n",
        "\n",
        "# Read each CSV file, standardize column names, and store in a list\n",
        "dataframes = [standardize_columns(pd.read_csv(file)) for file in csv_files]\n",
        "\n",
        "# Concatenate all DataFrames\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Save the combined DataFrame to a new CSV file\n",
        "combined_csv_path = '/content/combined_file.csv'\n",
        "combined_df.to_csv(combined_csv_path, index=False)\n",
        "\n",
        "print(f\"CSV files have been combined successfully. The combined file is saved at: {combined_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ2KPINcxWLA"
      },
      "outputs": [],
      "source": [
        "df4=pd.read_csv(\"/content/combined_file.csv\")\n",
        "df4.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUqdpHqf63Fr"
      },
      "outputs": [],
      "source": [
        "# prompt: Using dataframe df4: print uique values of age\n",
        "\n",
        "df4['ethnicity'].unique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfomiBUs61jL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APzDDhAB1D0U"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Assuming 'data' is your DataFrame\n",
        "encoder = LabelEncoder()\n",
        "df4['ethnicity'] = encoder.fit_transform(d4['ethnicity'])\n",
        "\n",
        "# Display the first few rows to verify the transformation\n",
        "print(df4.head())\n",
        "df4.to_csv('Data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYXGbFdmxfjF"
      },
      "outputs": [],
      "source": [
        "df4.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ni0sf7baxz9i"
      },
      "outputs": [],
      "source": [
        "df4.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvLXypVn6X9G"
      },
      "outputs": [],
      "source": [
        "plt.pie(df4['class/asd traits'].value_counts().values, autopct='%1.1f%%')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06MQoauV127H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (accuracy_score, roc_auc_score, f1_score,\n",
        "                             precision_score, recall_score, matthews_corrcoef,\n",
        "                             cohen_kappa_score, log_loss)\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.impute import SimpleImputer # Import the imputer\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/Data.csv')\n",
        "# Split the data into features (X) and target (y)\n",
        "X = data.drop('class/asd traits', axis=1)\n",
        "y = data['class/asd traits']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Handle missing values using imputation\n",
        "imputer = SimpleImputer(strategy='mean') # Create an imputer to fill missing values, you can use other strategies like 'median', 'most_frequent' etc.\n",
        "X_train_imputed = imputer.fit_transform(X_train) # Fit and transform on the training data\n",
        "X_test_imputed = imputer.transform(X_test) # Transform the test data using the same imputer\n",
        "\n",
        "# Standardize the data after imputation\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "X_test_scaled = scaler.transform(X_test_imputed)\n",
        "\n",
        "# Define classifiers\n",
        "classifiers = {\n",
        "    'MLP': MLPClassifier(max_iter=500),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'SVM': SVC(probability=True),\n",
        "    'LDA': LinearDiscriminantAnalysis(),\n",
        "    'KNN': KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "# Evaluate and compare classifiers\n",
        "results = {}\n",
        "\n",
        "for clf_name, clf in classifiers.items():\n",
        "    # Train the classifier\n",
        "    clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "    # Evaluate different metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, clf.predict_proba(X_test_scaled)[:, 1])\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    mcc = matthews_corrcoef(y_test, y_pred)\n",
        "    kappa = cohen_kappa_score(y_test, y_pred)\n",
        "    logloss = log_loss(y_test, clf.predict_proba(X_test_scaled)[:, 1])\n",
        "\n",
        "    # Store results\n",
        "    results[clf_name] = {\n",
        "        'Accuracy': accuracy,\n",
        "        'ROC AUC': roc_auc,\n",
        "        'F1-Score': f1,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'MCC': mcc,\n",
        "        'Kappa': kappa,\n",
        "        'Log Loss': logloss\n",
        "    }\n",
        "\n",
        "# Display results\n",
        "print(\"Classification Results:\")\n",
        "for clf_name, metrics in results.items():\n",
        "    print(f\"{clf_name}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Optionally, convert the results to a DataFrame for better visualization\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"Classification Results in DataFrame format:\")\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZie9W8mHV6x"
      },
      "outputs": [],
      "source": [
        "df4.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "May5ihxn3CpX"
      },
      "outputs": [],
      "source": [
        "#stack model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer # Import SimpleImputer for handling missing values\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are your training and testing data\n",
        "X_train_base, X_meta, y_train_base, y_meta = train_test_split(X_train, y_train, test_size=0.5, random_state=42)\n",
        "\n",
        "# Handle missing values using imputation\n",
        "imputer = SimpleImputer(strategy='mean') # Create an imputer to fill missing values\n",
        "X_train_base_imputed = imputer.fit_transform(X_train_base) # Fit and transform on the training data\n",
        "X_meta_imputed = imputer.transform(X_meta) # Transform the meta data using the same imputer\n",
        "X_test_imputed = imputer.transform(X_test) # Transform the test data using the same imputer\n",
        "\n",
        "# Train base models\n",
        "mlp = MLPClassifier(max_iter=1000, learning_rate_init=0.001)\n",
        "ada = AdaBoostClassifier()\n",
        "mlp.fit(X_train_base_imputed, y_train_base) # Use imputed data for training\n",
        "ada.fit(X_train_base_imputed, y_train_base) # Use imputed data for training\n",
        "\n",
        "# Make predictions on meta data\n",
        "mlp_pred = mlp.predict(X_meta_imputed) # Use imputed data for prediction\n",
        "ada_pred = ada.predict(X_meta_imputed) # Use imputed data for prediction\n",
        "\n",
        "# Create meta features\n",
        "meta_features = np.column_stack((mlp_pred, ada_pred))\n",
        "\n",
        "# Train meta-learner\n",
        "meta_learner = RandomForestClassifier()\n",
        "meta_learner.fit(meta_features, y_meta)\n",
        "\n",
        "# Make predictions on test data\n",
        "mlp_test_pred = mlp.predict(X_test_imputed) # Use imputed data for prediction\n",
        "ada_test_pred = ada.predict(X_test_imputed) # Use imputed data for prediction\n",
        "meta_features_test = np.column_stack((mlp_test_pred, ada_test_pred))\n",
        "final_pred = meta_learner.predict(meta_features_test)\n",
        "\n",
        "# Evaluate final predictions\n",
        "accuracy = accuracy_score(y_test, final_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBvu0gKs4ubA"
      },
      "outputs": [],
      "source": [
        "#hyperparameter testing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are your training and testing data\n",
        "X_train_base, X_meta, y_train_base, y_meta = train_test_split(X_train, y_train, test_size=0.5, random_state=42)\n",
        "\n",
        "# Handle missing values using imputation\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_base_imputed = imputer.fit_transform(X_train_base)\n",
        "X_meta_imputed = imputer.transform(X_meta)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Train base models\n",
        "mlp = MLPClassifier(max_iter=1000, learning_rate_init=0.001)\n",
        "ada = AdaBoostClassifier()\n",
        "mlp.fit(X_train_base_imputed, y_train_base)\n",
        "ada.fit(X_train_base_imputed, y_train_base)\n",
        "\n",
        "# Make predictions on meta data\n",
        "mlp_pred = mlp.predict(X_meta_imputed)\n",
        "ada_pred = ada.predict(X_meta_imputed)\n",
        "\n",
        "# Create meta features\n",
        "meta_features = np.column_stack((mlp_pred, ada_pred))\n",
        "\n",
        "# Define hyperparameters to search for Random Forest (meta-learner)\n",
        "rf_params = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV for Random Forest (meta-learner)\n",
        "rf_grid_search = GridSearchCV(RandomForestClassifier(), rf_params, cv=5)\n",
        "rf_grid_search.fit(meta_features, y_meta)\n",
        "\n",
        "# Get the best hyperparameters for Random Forest\n",
        "best_rf_params = rf_grid_search.best_params_\n",
        "\n",
        "# Print the best hyperparameters for Random Forest\n",
        "print(\"Best Hyperparameters for Random Forest:\")\n",
        "for param, value in best_rf_params.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "# Train Random Forest (meta-learner) with best parameters\n",
        "rf_best = RandomForestClassifier(**best_rf_params)\n",
        "rf_best.fit(meta_features, y_meta)\n",
        "\n",
        "# Make predictions on test data\n",
        "mlp_test_pred = mlp.predict(X_test_imputed)\n",
        "ada_test_pred = ada.predict(X_test_imputed)\n",
        "meta_features_test = np.column_stack((mlp_test_pred, ada_test_pred))\n",
        "final_pred = rf_best.predict(meta_features_test)\n",
        "\n",
        "# Evaluate final predictions\n",
        "accuracy = accuracy_score(y_test, final_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKzNHrLp4uHX"
      },
      "outputs": [],
      "source": [
        "#Feature importance\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are your training and testing data\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Sort feature importances in descending order\n",
        "sorted_indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(X_train.shape[1]), importances[sorted_indices], align=\"center\")\n",
        "plt.xticks(range(X_train.shape[1]), sorted_indices)\n",
        "plt.xlabel(\"Feature index\")\n",
        "plt.ylabel(\"Feature importance\")\n",
        "plt.title(\"Feature Importances\")\n",
        "plt.show()\n",
        "\n",
        "# Print the importance of the \"autism in immediate family\" feature\n",
        "print(f\"Importance of 'autism in immediate family': {importances[feature_index]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFFaXbVoDIIx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def standardize_columns(df):\n",
        "    df.columns = df.columns.str.lower().str.strip().str.replace('/', '_')  # Convert column names to lowercase, strip whitespace, and replace '/' with '_'\n",
        "    return df\n",
        "\n",
        "# Apply the function to your dataframes\n",
        "df = standardize_columns(df)\n",
        "df1 = standardize_columns(df1)\n",
        "df2 = standardize_columns(df2)\n",
        "df3 = standardize_columns(df3)\n",
        "\n",
        "# Drop the 'Who_completed_the_test' column\n",
        "#df.drop('who_completed_the_test', axis=1, inplace=True)\n",
        "# Combine the DataFrames\n",
        "combined_df = pd.concat([df, df1, df2, df3], ignore_index=True)\n",
        "# Assuming 'data' is your DataFrame\n",
        "encoder = LabelEncoder()\n",
        "combined_df['ethnicity'] = encoder.fit_transform(combined_df['ethnicity'])\n",
        "\n",
        "# Save the combined DataFrame to a CSV file\n",
        "combined_csv_path = 'combined_data.csv'\n",
        "combined_df.to_csv(combined_csv_path, index=False)\n",
        "\n",
        "print(f\"Combined DataFrame saved as CSV: {combined_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hjv1tVRoEiGT"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/combined_data.csv')\n",
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkrh9u-SFYty"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCsSp2x2Dbal"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are your training and testing data\n",
        "X_train_base, X_meta, y_train_base, y_meta = train_test_split(X_train, y_train, test_size=0.5, random_state=42)\n",
        "\n",
        "# Handle missing values using imputation\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_base_imputed = imputer.fit_transform(X_train_base)\n",
        "X_meta_imputed = imputer.transform(X_meta)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Train base models\n",
        "mlp = MLPClassifier(max_iter=1000, learning_rate_init=0.001)\n",
        "ada = AdaBoostClassifier()\n",
        "mlp.fit(X_train_base_imputed, y_train_base)\n",
        "ada.fit(X_train_base_imputed, y_train_base)\n",
        "\n",
        "# Make predictions on meta data\n",
        "mlp_pred = mlp.predict(X_meta_imputed)\n",
        "ada_pred = ada.predict(X_meta_imputed)\n",
        "\n",
        "# Create meta features\n",
        "meta_features = np.column_stack((mlp_pred, ada_pred))\n",
        "\n",
        "# Define hyperparameters to search for Random Forest (meta-learner)\n",
        "rf_params = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV for Random Forest (meta-learner)\n",
        "rf_grid_search = GridSearchCV(RandomForestClassifier(), rf_params, cv=5)\n",
        "rf_grid_search.fit(meta_features, y_meta)\n",
        "\n",
        "# Get the best hyperparameters for Random Forest\n",
        "best_rf_params = rf_grid_search.best_params_\n",
        "\n",
        "# Print the best hyperparameters for Random Forest\n",
        "print(\"Best Hyperparameters for Random Forest:\")\n",
        "for param, value in best_rf_params.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "# Train Random Forest (meta-learner) with best parameters\n",
        "rf_best = RandomForestClassifier(**best_rf_params)\n",
        "rf_best.fit(meta_features, y_meta)\n",
        "\n",
        "# Make predictions on test data\n",
        "mlp_test_pred = mlp.predict(X_test_imputed)\n",
        "ada_test_pred = ada.predict(X_test_imputed)\n",
        "meta_features_test = np.column_stack((mlp_test_pred, ada_test_pred))\n",
        "final_pred = rf_best.predict(meta_features_test)\n",
        "\n",
        "# Evaluate final predictions\n",
        "accuracy = accuracy_score(y_test, final_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwX2n6OtUIrZ"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/combined_data.csv')\n",
        "data.drop('cv',inplace=True,axis=1)\n",
        "data.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rXnKS12Irli"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (accuracy_score, roc_auc_score, f1_score,\n",
        "                             precision_score, recall_score, matthews_corrcoef,\n",
        "                             cohen_kappa_score, log_loss)\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = data.drop('class_asd traits', axis=1)\n",
        "y = data['class_asd traits']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Handle missing values using imputation\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Standardize the data after imputation\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "X_test_scaled = scaler.transform(X_test_imputed)\n",
        "\n",
        "# Define the AdaBoost classifier\n",
        "adaboost = AdaBoostClassifier()\n",
        "\n",
        "# Train the AdaBoost classifier on the scaled data\n",
        "adaboost.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_adaboost = adaboost.predict(X_test_scaled)\n",
        "y_pred_proba_adaboost = adaboost.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)\n",
        "roc_auc_adaboost = roc_auc_score(y_test, y_pred_proba_adaboost)\n",
        "f1_adaboost = f1_score(y_test, y_pred_adaboost)\n",
        "precision_adaboost = precision_score(y_test, y_pred_adaboost)\n",
        "recall_adaboost = recall_score(y_test, y_pred_adaboost)\n",
        "mcc_adaboost = matthews_corrcoef(y_test, y_pred_adaboost)\n",
        "kappa_adaboost = cohen_kappa_score(y_test, y_pred_adaboost)\n",
        "logloss_adaboost = log_loss(y_test, y_pred_proba_adaboost)\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(\"AdaBoost Classifier Evaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_adaboost:.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc_adaboost:.4f}\")\n",
        "print(f\"F1-Score: {f1_adaboost:.4f}\")\n",
        "print(f\"Precision: {precision_adaboost:.4f}\")\n",
        "print(f\"Recall: {recall_adaboost:.4f}\")\n",
        "print(f\"MCC: {mcc_adaboost:.4f}\")\n",
        "print(f\"Kappa: {kappa_adaboost:.4f}\")\n",
        "print(f\"Log Loss: {logloss_adaboost:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_-2qv6XVEpV"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TrImFqlWAOn"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Train the AdaBoost classifier\n",
        "ada_boost = AdaBoostClassifier()\n",
        "ada_boost.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Serialize the model using joblib\n",
        "joblib.dump(ada_boost, 'ASD_model.joblib')\n",
        "\n",
        "# Load the serialized model\n",
        "loaded_model = joblib.load('ASD_model.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3AUHXO3JS1F"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Train the AdaBoost classifier\n",
        "ada_boost = AdaBoostClassifier()\n",
        "ada_boost.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Save the trained model to a file\n",
        "with open('ASD_model.pkl', 'wb') as model_file:\n",
        "    pickle.dump(ada_boost, model_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzNdwWdANVdu"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIo6XvGkNpHb"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2SWqY5FSeeC"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Example categorical columns for training\n",
        "ethnicities = ['middle eastern', 'White European', 'Hispanic', 'black', 'asian',\n",
        "               'south asian', 'Native Indian', 'Others', 'Latino', 'mixed',\n",
        "               'Pacifica', 'Middle Eastern', 'White-European', 'Black',\n",
        "               'South Asian', 'Asian', 'Pasifika', 'Turkish', 'Middle Eastern ',\n",
        "               'others']\n",
        "\n",
        "# Initialize label encoders\n",
        "ethnicity_encoder = LabelEncoder()\n",
        "ethnicity_encoded = ethnicity_encoder.fit_transform(ethnicities)\n",
        "\n",
        "# Save the ethnicity encoder\n",
        "with open('ethnicity_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(ethnicity_encoder, f)\n",
        "\n",
        "# Initialize and save other label encoders similarly\n",
        "yes_no_encoder = LabelEncoder()\n",
        "yes_no_encoded = yes_no_encoder.fit_transform(['Yes', 'No'])\n",
        "\n",
        "with open('yes_no_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(yes_no_encoder, f)\n",
        "\n",
        "sex_encoder = LabelEncoder()\n",
        "sex_encoded = sex_encoder.fit_transform(['M', 'F'])\n",
        "\n",
        "with open('sex_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(sex_encoder, f)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
